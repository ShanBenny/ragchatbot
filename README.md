C-CARES RAG Chatbot (Ollama + LangChain + Streamlit)

This project is an intelligent chatbot that answers questions based on the C-CARES User Manual.
It uses Retrieval-Augmented Generation (RAG) with LangChain, Chroma Vector Database, and Ollama (LLaMA-3) for efficient document retrieval and contextual answering.
The frontend is built with Streamlit, offering a clean and professional chat interface.

ğŸ§  Features

Conversational chatbot with context memory

Accurate, document-based answers using RAG

Local embeddings and vector storage with ChromaDB

Integration with Ollama LLaMA-3 model

Professional Streamlit UI with CDAC branding

Displays previous chat history

Persistent vector store for offline use

ğŸ“ Project Structure
ragchatbot/
â”œâ”€â”€ app.py                 # Streamlit frontend
â”œâ”€â”€ rag_pipeline.py        # Core RAG backend logic
â”œâ”€â”€ data/
â”‚   â””â”€â”€ C-CARES_User_Manual_VN1.0.docx  # Input document
â”œâ”€â”€ vectorstore/           # Chroma database files
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # Project documentation

âš™ï¸ Installation Steps

Clone the Repository

git clone https://github.com/yourusername/ragchatbot.git
cd ragchatbot


Create and Activate Virtual Environment

python -m venv venv
venv\Scripts\activate      # on Windows
source venv/bin/activate   # on Linux/Mac


Install Dependencies

pip install -r requirements.txt


Start Ollama Server

Install Ollama

Run:

ollama run llama3


Index the Document

python -c "from rag_pipeline import load_and_index; load_and_index()"


Run the Streamlit App

streamlit run app.py

ğŸ§© How It Works

The C-CARES user manual (.docx) is split into smaller text chunks.

Each chunk is converted into vector embeddings using the OllamaEmbeddings model.

These embeddings are stored in a Chroma vector database for fast retrieval.

When a user asks a question, the system retrieves the most relevant document chunks.

The LLaMA-3 model (via Ollama) generates an answer based on both the question and retrieved context.

The conversation history is preserved for contextual follow-up questions.

ğŸ“Š Architecture Overview
User â†’ Streamlit UI â†’ LangChain Pipeline â†’ ChromaDB Retriever â†’ Ollama LLaMA-3 â†’ Response


For a visual overview, see the architecture diagram in /docs/architecture.png.

ğŸ§° Tech Stack

Component	Tool
Frontend	Streamlit
Backend	LangChain
Embeddings	OllamaEmbeddings
Vector Store	ChromaDB
LLM	LLaMA-3 (via Ollama)
Document Loader	Docx2txtLoader
ğŸ§‘â€ğŸ’» Author
Developed by Shan Benny
Apprenticeship Project â€“ C-DAC Bangalore
Group: Big Data and Machine Learning

ğŸ“„ License

This project is developed for internal learning and research use under C-DAC guidelines.
